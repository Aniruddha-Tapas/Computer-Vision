{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Optical Character Recognition using K Nearest Neighbours\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Problem:\n",
    "We will use our knowledge on kNN to build a basic OCR application. The goal of our OCR application is to read and recognize the handwritten digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Dataset:\n",
    "The dataset required to deal with the problem is already available in OpenCV docs which is an image file named as digits.png (in the folder opencv/samples/python2/data/) which has 5000 handwritten digits (500 for each digit). Each digit is a 20x20 image.\n",
    "\n",
    "You can download the digits.png file from https://github.com/Itseez/opencv/blob/master/samples/data/digits.png by just right-clicking raw and saving the link in your project directory.\n",
    "\n",
    "After downloading the file we have to split our data into train_data and test_data. So our first step is to split this image into 5000 different digits. For each digit, we flatten it into a single row with 400 pixels. That is our feature set, ie intensity values of all pixels. It is the simplest feature set we can create. We use first 250 samples of each digit as train_data, and next 250 samples as test_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prcoessing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function KNearest_create>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "#cv2.__version__\n",
    "cv2.ml.KNearest_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "cv2.ml.KNearest_create()\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv2.imread('datasets/digits.png')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Now we split the image to 5000 cells, each 20x20 size\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]\n",
    "\n",
    "# Make it into a Numpy array. It size will be (50,100,20,20)\n",
    "x = np.array(cells)\n",
    "\n",
    "# Now we prepare train_data and test_data.\n",
    "train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    "test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    "\n",
    "# Create labels for train and test data\n",
    "k = np.arange(10)\n",
    "train_labels = np.repeat(k,250)[:,np.newaxis]\n",
    "test_labels = train_labels.copy()\n",
    "\n",
    "# Initiate kNN, train the data, then test it with test data for k=1\n",
    "knn = cv2.KNearest()\n",
    "knn.train(train,train_labels)\n",
    "ret,result,neighbours,dist = knn.find_nearest(test,k=5)\n",
    "\n",
    "# Now we check the accuracy of classification\n",
    "# For that, compare the result with test_labels and check which are wrong\n",
    "matches = result==test_labels\n",
    "correct = np.count_nonzero(matches)\n",
    "accuracy = correct*100.0/result.size\n",
    "print accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So our basic OCR app is ready. This particular example gave me an accuracy of 91%. One option improve accuracy is to add more data for training, especially the wrong ones. So instead of finding this training data everytime I start application, I better save it, so that next time, I directly read this data from a file and start classification. You can do it with the help of some Numpy functions like np.savetxt, np.savez, np.load etc. Please check their docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savez('knn_data.npz',train=train, train_labels=train_labels)\n",
    "# Now load the data\n",
    "with np.load('knn_data.npz') as data:\n",
    "print data.files\n",
    "train = data['train']\n",
    "train_labels = data['train_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## OCR of English Alphabets\n",
    "\n",
    "Next we will do the same for English alphabets, but there is a slight change in data and feature set. Here, instead of images, OpenCV comes with a data file, letter-recognition.data in opencv/samples/cpp/ folder. If you open it, you will see 20000 lines which may, on first sight, look like garbage. Actually, in each row, first column is an alphabet which is our label. Next 16 numbers following it are its different features. These features are obtained from UCI Machine Learning Repository. You can find the details of these features and can download the dataset from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/.\n",
    "\n",
    "There are 20000 samples available, so we take first 10000 data as training samples and remaining 10000 as test samples. We should change the alphabets to ascii characters because we can't work with alphabets directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data, converters convert the letter to a number\n",
    "data= np.loadtxt('letter-recognition.data', dtype= 'float32', delimiter = ',',\n",
    "converters= {0: lambda ch: ord(ch)-ord('A')})\n",
    "\n",
    "# split the data to two, 10000 each for train and test\n",
    "train, test = np.vsplit(data,2)\n",
    "\n",
    "# split trainData and testData to features and responses\n",
    "responses, trainData = np.hsplit(train,[1])\n",
    "labels, testData = np.hsplit(test,[1])\n",
    "\n",
    "# Initiate the kNN, classify, measure accuracy.\n",
    "knn = cv2.KNearest()\n",
    "knn.train(trainData, responses)\n",
    "ret, result, neighbours, dist = knn.find_nearest(testData, k=5)\n",
    "correct = np.count_nonzero(result == labels)\n",
    "accuracy = correct*100.0/10000\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It gives me an accuracy of 93.22%. Again, if you want to increase accuracy, you can iteratively add error data in each\n",
    "level.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
